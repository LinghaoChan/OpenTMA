# OpenTMA: Open Text-Motion Alignment Project

## âœ¨ Quick Introduction

OpenTMA is a project that aims to provide a simple and efficient way to align text and motion data. It is designed to be easy to use and flexible, allowing users to align text and motion data in the latent space. 


## Todo List


- [x] Release the OpenTMA training.
- [ ] Release the OpenTMA checkpoints.
- [ ] Support PyPI (`pip install opentma`).

## ðŸš€ Quick start

### Installation

```bash
pip install opentma
```

### Usage

```python
# Load text and motion data
import torch
from transformers import AutoTokenizer, AutoModel
from tma.models.architectures.temos.textencoder.distillbert_actor import DistilbertActorAgnosticEncoder
from tma.models.architectures.temos.motionencoder.actor import ActorAgnosticEncoder
from collections import OrderedDict

modelpath = 'distilbert-base-uncased'

textencoder = DistilbertActorAgnosticEncoder(modelpath, num_layers=4)
motionencoder = ActorAgnosticEncoder(nfeats=126, vae = True, num_layers=4)

"""
load model here
"""

motion = torch.randn(1, 64, 126)    # B = 1, T = , D = , need normalization
lengths = [64]
print(textencoder(["a man is running"]).loc)
print(motionencoder(motion, lengths).loc)
```

## Test for Evaluation

Before running the code below, please revise the `retreival.sh` (like `path1` variable) file to set the correct path for the data. 

```bash
bash retreival.sh
```

## Model Training

### 1. Data Preparation

Our OpenTMA project supports three datasets: [HumanML3D](https://github.com/EricGuo5513/HumanML3D?tab=readme-ov-file#how-to-obtain-the-data), [Motion-X](https://motionx.deepdataspace.com/), and [UniMoCap](https://github.com/LinghaoChan/UniMoCap). 

<details>
  <summary><b> HumanML3D Data Preparation </b></summary>

Please following the instructions in the [HumanML3D](https://github.com/EricGuo5513/HumanML3D?tab=readme-ov-file#how-to-obtain-the-data) repository to download and preprocess the data. The data should be stored in the `./datasets/humanml3d` folder. The path tree should look like this:

```
./OpenTMR/datasets/humanml3d/
â”œâ”€â”€ all.txt
â”œâ”€â”€ Mean.npy
â”œâ”€â”€ new_joints/
â”œâ”€â”€ new_joint_vecs/
â”œâ”€â”€ Std.npy
â”œâ”€â”€ test.txt
â”œâ”€â”€ texts/
â”œâ”€â”€ train.txt
â”œâ”€â”€ train_val.txt
â””â”€â”€ val.txt
```

</details>


<details>
  <summary><b> Motion-X Data Preparation </b></summary>

Please following the instructions in the [Motion-X](https://github.com/IDEA-Research/Motion-X?tab=readme-ov-file#-dataset-download) project. And then please follow the [HumanTOMATO](https://github.com/IDEA-Research/HumanTOMATO/tree/main/src/tomato_represenation) repository to preprocess the data into `tomatao` format. The data should be stored in the `./datasets/Motion-X` folder. The path tree should look like this:

```
./OpenTMR/datasets/Motion-X
â”œâ”€â”€ mean_std
â”‚   â””â”€â”€ vector_623
â”‚       â”œâ”€â”€ mean.npy
â”‚       â””â”€â”€ std.npy
â”œâ”€â”€ motion_data
â”‚   â””â”€â”€ vector_623
â”‚       â”œâ”€â”€ aist/       (subset_*/*.npy)
â”‚       â”œâ”€â”€ animation/
â”‚       â”œâ”€â”€ dance/
â”‚       â”œâ”€â”€ EgoBody/
â”‚       â”œâ”€â”€ fitness/
â”‚       â”œâ”€â”€ game_motion/
â”‚       â”œâ”€â”€ GRAB/
â”‚       â”œâ”€â”€ HAA500/
â”‚       â”œâ”€â”€ humanml/
â”‚       â”œâ”€â”€ humman/
â”‚       â”œâ”€â”€ idea400/
â”‚       â”œâ”€â”€ kungfu/
â”‚       â”œâ”€â”€ music/
â”‚       â””â”€â”€ perform/
â”œâ”€â”€ split
â”‚   â”œâ”€â”€ all.txt
â”‚   â”œâ”€â”€ test.txt
â”‚   â”œâ”€â”€ train.txt
â”‚   â””â”€â”€ val.txt
â””â”€â”€ texts
    â”œâ”€â”€ semantic_texts
    â”‚   â”œâ”€â”€ aist/       (subset_*/*.txt)
    â”‚   â”œâ”€â”€ animation/
    â”‚   â”œâ”€â”€ dance/
    â”‚   â”œâ”€â”€ EgoBody/
    â”‚   â”œâ”€â”€ fitness/
    â”‚   â”œâ”€â”€ game_motion/
    â”‚   â”œâ”€â”€ GRAB/
    â”‚   â”œâ”€â”€ HAA500/
    â”‚   â”œâ”€â”€ humanml/
    â”‚   â”œâ”€â”€ humman/
    â”‚   â”œâ”€â”€ idea400/
    â”‚   â”œâ”€â”€ kungfu/
    â”‚   â”œâ”€â”€ music/
    â””â”€â”€â”€â””â”€â”€ perform/
```

</details>


<details>
  <summary><b> UniMoCap Data Preparation </b></summary>

Please following the instructions in the [UniMoCap](https://github.com/LinghaoChan/UniMoCap) repository to download and preprocess the data (HumanML3D, BABEL, and KIT-ML). The data should be stored in the `./datasets/UniMocap` folder. The path tree should look like this:

```
./OpenTMR/datasets/UniMocap
â”œâ”€â”€ all.txt
â”œâ”€â”€ Mean.npy
â”œâ”€â”€ new_joints/     (*.npy)
â”œâ”€â”€ new_joint_vecs/ (*.npy)
â”œâ”€â”€ Std.npy
â”œâ”€â”€ test.txt
â”œâ”€â”€ texts/          (*.txt)
â”œâ”€â”€ train.txt
â”œâ”€â”€ train_val.txt
â””â”€â”€ val.txt
```

</details>



### 2. Pretrained Checkpoints Used in the Evaluation 

Here, we provide some pre-traind checkpoints for the evaluation. Here are two methods to download the checkpoints:


<details>
<summary><b> Google Drive</b></summary>


Download the checkpoints from the [Google Drive](https://drive.google.com/drive/folders/1aWpJH4KTXsWnxG5MciLHXPXGBS7vWXf7?usp=share_link) and put them in the `./deps` folder. Please unzip the checkpoints via the following command:
```
unzip *.zip
```
Finally, the path tree should look like this:

```
./deps
â”œâ”€â”€ distilbert-base-uncased/
â”œâ”€â”€ glove/
â”œâ”€â”€ t2m/
â””â”€â”€ transforms/
```

</details>


<details>
<summary><b> Baidu Drive</b></summary>


Download the checkpoints from the [Baidu Drive](https://pan.baidu.com/s/1SIwGDX2aDWTR4hLhUHrPlw?pwd=evan ) (code: `evan`) and put them in the `./deps` folder. Please unzip the checkpoints via the following command:
```
tar â€“xvf deps.tar
```
Finally, the path tree should look like this:

```
./deps
â”œâ”€â”€ distilbert-base-uncased/
â”œâ”€â”€ glove/
â”œâ”€â”€ t2m/
â””â”€â”€ transforms/
```

</details>


### 3. Downloading Pretrained Checkpoints

We provide some pretrained checkpoints of OpenTMA for evaluation.
